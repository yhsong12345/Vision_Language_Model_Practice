# Training VLA with Temporal and World Models

This comprehensive guide covers the complete training process for Vision-Language-Action models with temporal modeling and world model capabilities, including recurrent state-space models, latent dynamics learning, imagination-based planning, and predictive control.

## Table of Contents

1. [Overview](#overview)
2. [Architecture for Temporal and World Models](#architecture-for-temporal-and-world-models)
3. [Temporal Modeling Components](#temporal-modeling-components)
4. [Stage 1: Temporal Encoder Training](#stage-1-temporal-encoder-training)
5. [Stage 2: World Model Training](#stage-2-world-model-training)
6. [Stage 3: Latent Dynamics Learning](#stage-3-latent-dynamics-learning)
7. [Stage 4: Imagination-Based Planning](#stage-4-imagination-based-planning)
8. [Stage 5: Model Predictive Control](#stage-5-model-predictive-control)
9. [Stage 6: Policy Learning with World Models](#stage-6-policy-learning-with-world-models)
10. [Advanced Topics](#advanced-topics)
11. [Deployment](#deployment)
12. [Evaluation and Benchmarks](#evaluation-and-benchmarks)

---

## Overview

### Temporal and World Model VLA Pipeline

```
+=======================================================================================+
|                    TEMPORAL AND WORLD MODEL VLA TRAINING PIPELINE                      |
+=======================================================================================+
|                                                                                        |
|  INPUT SEQUENCE                                                                        |
|  +-----------------------------------------------------------------------------------+ |
|  |  Visual Obs (t-H:t)  |  Proprioception (t-H:t)  |  Actions (t-H:t)  |  Language   | |
|  +-----------------------------------------------------------------------------------+ |
|                                          |                                             |
|                                          v                                             |
|  TEMPORAL ENCODERS                                                                     |
|  +-----------------------------------------------------------------------------------+ |
|  |  History Encoder  |  Temporal Transformer  |  Recurrent State  |  Context Encoder | |
|  +-----------------------------------------------------------------------------------+ |
|                                          |                                             |
|                                          v                                             |
|  WORLD MODEL (Latent Dynamics)                                                         |
|  +-----------------------------------------------------------------------------------+ |
|  |  RSSM (Recurrent State-Space Model)                                               | |
|  |  - Encoder: obs -> latent state                                                   | |
|  |  - Dynamics: (state, action) -> next_state (deterministic + stochastic)           | |
|  |  - Decoder: latent state -> obs reconstruction                                    | |
|  |  - Reward predictor: latent state -> expected reward                              | |
|  +-----------------------------------------------------------------------------------+ |
|                                          |                                             |
|                                          v                                             |
|  VLM BACKBONE (Language-Conditioned)                                                   |
|  +-----------------------------------------------------------------------------------+ |
|  |  Vision-Language Model with Temporal Context                                       | |
|  |  - Processes: "Pick up the red cube and place it on the plate"                    | |
|  |  - Integrates: Visual history + Temporal features + Language                       | |
|  +-----------------------------------------------------------------------------------+ |
|                                          |                                             |
|                                          v                                             |
|  PLANNING AND CONTROL                                                                  |
|  +-----------------------------------------------------------------------------------+ |
|  |  Imagination-Based Planning  |  Model Predictive Control  |  Policy Network       | |
|  |  (Sample trajectories)       |  (Optimize actions)        |  (Direct prediction)   | |
|  +-----------------------------------------------------------------------------------+ |
|                                                                                        |
|  OUTPUT: Action sequence with predicted future states                                  |
+=======================================================================================+
```

### Key Benefits of World Models

| Aspect | Without World Model | With World Model |
|--------|-------------------|------------------|
| Sample Efficiency | Low (100K+ real samples) | High (10K real + imagined) |
| Planning | Reactive only | Predictive lookahead |
| Safety | Reactive collision avoidance | Proactive risk prediction |
| Generalization | Limited to seen states | Interpolation in latent space |
| Multi-step Reasoning | Difficult | Natural through imagination |
| Training Data | Requires many environment interactions | Imagination supplements real data |

---

## Architecture for Temporal and World Models

### TemporalWorldModelVLA Configuration

```python
from model.world_model import TemporalWorldModelVLA, TemporalWorldModelConfig

@dataclass
class TemporalWorldModelConfig:
    # Vision-Language Model
    vlm_backbone: str = "Qwen/Qwen2-1.5B-Instruct"
    vision_encoder: str = "google/siglip-base-patch16-224"

    # Temporal Configuration
    history_length: int = 16          # Past observations
    prediction_horizon: int = 16      # Future prediction steps
    context_dim: int = 256            # Temporal context dimension

    # World Model (RSSM) Configuration
    state_dim: int = 256              # Deterministic state dimension
    stochastic_dim: int = 32          # Stochastic state dimension
    hidden_dim: int = 512             # GRU hidden dimension
    num_layers: int = 2               # Number of GRU layers

    # Latent Dynamics
    dynamics_type: str = "rssm"       # rssm, gru, transformer
    use_discrete_latent: bool = False # DreamerV3-style discrete latents
    num_categories: int = 32          # Categories for discrete latent
    num_classes: int = 32             # Classes per category

    # Decoder Configuration
    decoder_type: str = "cnn"         # cnn, transformer, diffusion
    reconstruction_loss: str = "mse"  # mse, perceptual, adversarial

    # Action Configuration
    action_dim: int = 7
    action_chunk_size: int = 8

    # Planning Configuration
    planning_horizon: int = 15        # Steps to plan ahead
    num_action_samples: int = 1000    # CEM samples
    top_k: int = 100                  # Top actions for CEM
    num_iterations: int = 5           # CEM iterations

    # Training
    imagination_ratio: float = 0.5    # Fraction of imagined trajectories
    kl_weight: float = 1.0            # KL divergence weight
    reconstruction_weight: float = 1.0
    reward_weight: float = 1.0
    free_nats: float = 3.0            # Free bits for KL
```

### Model Implementation

```python
from model.world_model import RSSM, TemporalEncoder, WorldModelDecoder

class TemporalWorldModelVLA(nn.Module):
    """
    VLA model with temporal encoding and world model capabilities.

    Architecture:
    1. Temporal Encoder: Process observation history
    2. RSSM World Model: Learn latent dynamics
    3. VLM Backbone: Language-conditioned reasoning
    4. Planner: Imagination-based action selection
    """

    def __init__(self, config: TemporalWorldModelConfig):
        super().__init__()
        self.config = config

        # Vision encoder
        self.vision_encoder = VisionEncoder(
            model_name=config.vision_encoder,
            output_dim=config.hidden_dim,
        )

        # Temporal encoder
        self.temporal_encoder = TemporalEncoder(
            input_dim=config.hidden_dim,
            context_dim=config.context_dim,
            history_length=config.history_length,
            encoder_type="transformer",  # transformer, lstm, tcn
        )

        # World model (RSSM)
        self.world_model = RSSM(
            obs_dim=config.hidden_dim,
            action_dim=config.action_dim,
            state_dim=config.state_dim,
            stochastic_dim=config.stochastic_dim,
            hidden_dim=config.hidden_dim,
            num_layers=config.num_layers,
        )

        # Decoders
        self.obs_decoder = WorldModelDecoder(
            state_dim=config.state_dim + config.stochastic_dim,
            output_dim=config.hidden_dim,
            decoder_type=config.decoder_type,
        )

        self.reward_predictor = RewardPredictor(
            state_dim=config.state_dim + config.stochastic_dim,
            hidden_dim=256,
        )

        self.continue_predictor = ContinuePredictor(
            state_dim=config.state_dim + config.stochastic_dim,
            hidden_dim=256,
        )

        # VLM backbone
        self.vlm = VLMModel(
            llm_model_name=config.vlm_backbone,
            vision_dim=config.hidden_dim + config.context_dim,
        )

        # Policy head (for actor-critic)
        self.actor = GaussianMLPActionHead(
            input_dim=config.state_dim + config.stochastic_dim,
            action_dim=config.action_dim,
            hidden_dim=256,
        )

        self.critic = nn.Sequential(
            nn.Linear(config.state_dim + config.stochastic_dim, 256),
            nn.ELU(),
            nn.Linear(256, 256),
            nn.ELU(),
            nn.Linear(256, 1),
        )

    def forward(
        self,
        images: torch.Tensor,           # (B, T, C, H, W)
        actions: torch.Tensor,          # (B, T, action_dim)
        instruction: str,
        imagine_horizon: int = 0,
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass with optional imagination.

        Args:
            images: Visual observation sequence
            actions: Action sequence
            instruction: Language instruction
            imagine_horizon: Steps to imagine into future

        Returns:
            - latent_states: Encoded latent states
            - predicted_actions: Policy output
            - imagined_states: Future state predictions (if imagine_horizon > 0)
        """
        B, T, C, H, W = images.shape

        # Encode observations
        obs_features = []
        for t in range(T):
            feat = self.vision_encoder(images[:, t])
            obs_features.append(feat)
        obs_features = torch.stack(obs_features, dim=1)  # (B, T, hidden_dim)

        # Temporal encoding
        temporal_context = self.temporal_encoder(obs_features, actions)

        # World model encoding
        latent_states = self.world_model.encode_sequence(obs_features, actions)

        # VLM processing with temporal context
        combined_features = torch.cat([
            latent_states["posterior_state"],  # Current state
            temporal_context,                   # Temporal context
        ], dim=-1)

        vlm_features = self.vlm(
            visual_features=combined_features,
            instruction=instruction,
        )

        # Policy prediction
        action_mean, action_std = self.actor(latent_states["posterior_state"][:, -1])

        output = {
            "latent_states": latent_states,
            "action_mean": action_mean,
            "action_std": action_std,
            "vlm_features": vlm_features,
        }

        # Imagination (future prediction)
        if imagine_horizon > 0:
            imagined = self.imagine(
                latent_states["posterior_state"][:, -1],
                latent_states["posterior_stochastic"][:, -1],
                horizon=imagine_horizon,
            )
            output["imagined_states"] = imagined

        return output

    def imagine(
        self,
        state: torch.Tensor,
        stochastic: torch.Tensor,
        horizon: int,
        policy: nn.Module = None,
    ) -> Dict[str, torch.Tensor]:
        """
        Imagine future trajectories using the world model.

        Args:
            state: Current deterministic state
            stochastic: Current stochastic state
            horizon: Number of steps to imagine
            policy: Policy to sample actions (if None, use self.actor)
        """
        if policy is None:
            policy = self.actor

        imagined_states = []
        imagined_stochastics = []
        imagined_rewards = []
        imagined_continues = []
        imagined_actions = []

        current_state = state
        current_stochastic = stochastic

        for t in range(horizon):
            # Full state
            full_state = torch.cat([current_state, current_stochastic], dim=-1)

            # Sample action from policy
            action_mean, action_std = policy(full_state)
            action = action_mean + torch.randn_like(action_std) * action_std

            # Predict next state
            next_state, next_stochastic = self.world_model.imagine_step(
                current_state, current_stochastic, action
            )

            # Predict reward and continue
            next_full_state = torch.cat([next_state, next_stochastic], dim=-1)
            reward = self.reward_predictor(next_full_state)
            cont = self.continue_predictor(next_full_state)

            # Store
            imagined_states.append(next_state)
            imagined_stochastics.append(next_stochastic)
            imagined_rewards.append(reward)
            imagined_continues.append(cont)
            imagined_actions.append(action)

            # Update current state
            current_state = next_state
            current_stochastic = next_stochastic

        return {
            "states": torch.stack(imagined_states, dim=1),
            "stochastics": torch.stack(imagined_stochastics, dim=1),
            "rewards": torch.stack(imagined_rewards, dim=1),
            "continues": torch.stack(imagined_continues, dim=1),
            "actions": torch.stack(imagined_actions, dim=1),
        }
```

---

## Temporal Modeling Components

### History Encoder

```python
class HistoryEncoder(nn.Module):
    """
    Encode observation-action history into temporal context.

    Methods:
    1. Transformer: Self-attention over history
    2. LSTM/GRU: Recurrent encoding
    3. TCN: Temporal Convolutional Network
    4. Mamba: State-space model (efficient)
    """

    def __init__(
        self,
        obs_dim: int,
        action_dim: int,
        context_dim: int,
        max_len: int = 16,
        encoder_type: str = "transformer",
    ):
        super().__init__()
        self.encoder_type = encoder_type

        # Input projection
        self.obs_proj = nn.Linear(obs_dim, context_dim)
        self.action_proj = nn.Linear(action_dim, context_dim)

        # Positional encoding
        self.pos_encoding = SinusoidalPositionalEncoding(context_dim, max_len)

        if encoder_type == "transformer":
            self.encoder = nn.TransformerEncoder(
                nn.TransformerEncoderLayer(
                    d_model=context_dim,
                    nhead=8,
                    dim_feedforward=context_dim * 4,
                    dropout=0.1,
                    batch_first=True,
                ),
                num_layers=4,
            )
        elif encoder_type == "lstm":
            self.encoder = nn.LSTM(
                input_size=context_dim * 2,
                hidden_size=context_dim,
                num_layers=2,
                batch_first=True,
                bidirectional=True,
            )
            self.output_proj = nn.Linear(context_dim * 2, context_dim)
        elif encoder_type == "tcn":
            self.encoder = TemporalConvNet(
                num_inputs=context_dim * 2,
                num_channels=[context_dim] * 4,
                kernel_size=3,
                dropout=0.1,
            )
        elif encoder_type == "mamba":
            self.encoder = Mamba(
                d_model=context_dim * 2,
                d_state=16,
                d_conv=4,
                expand=2,
            )
            self.output_proj = nn.Linear(context_dim * 2, context_dim)

    def forward(
        self,
        observations: torch.Tensor,  # (B, T, obs_dim)
        actions: torch.Tensor,       # (B, T, action_dim)
    ) -> torch.Tensor:
        """
        Returns: Temporal context (B, T, context_dim)
        """
        B, T, _ = observations.shape

        # Project inputs
        obs_feat = self.obs_proj(observations)
        action_feat = self.action_proj(actions)

        # Combine observation and action
        combined = torch.cat([obs_feat, action_feat], dim=-1)  # (B, T, context_dim*2)

        if self.encoder_type == "transformer":
            # Add positional encoding (only to obs_feat for transformer)
            obs_feat = obs_feat + self.pos_encoding(obs_feat)

            # Self-attention with causal mask
            mask = self._generate_causal_mask(T, combined.device)
            context = self.encoder(obs_feat, mask=mask)

        elif self.encoder_type == "lstm":
            context, _ = self.encoder(combined)
            context = self.output_proj(context)

        elif self.encoder_type == "tcn":
            # TCN expects (B, C, T)
            combined = combined.permute(0, 2, 1)
            context = self.encoder(combined)
            context = context.permute(0, 2, 1)
            context = context[..., :obs_feat.shape[-1]]  # Trim to context_dim

        elif self.encoder_type == "mamba":
            context = self.encoder(combined)
            context = self.output_proj(context)

        return context

    def _generate_causal_mask(self, size: int, device: torch.device) -> torch.Tensor:
        """Generate causal attention mask."""
        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask


class TemporalTransformer(nn.Module):
    """
    Transformer for temporal action sequence modeling.

    Supports:
    - Causal (autoregressive) generation
    - Bidirectional encoding
    - Cross-attention to observations
    """

    def __init__(
        self,
        d_model: int = 512,
        num_heads: int = 8,
        num_layers: int = 6,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        use_causal_mask: bool = True,
    ):
        super().__init__()
        self.d_model = d_model
        self.use_causal_mask = use_causal_mask

        # Encoder layers
        self.encoder_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=num_heads,
                dim_feedforward=dim_feedforward,
                dropout=dropout,
                batch_first=True,
            )
            for _ in range(num_layers)
        ])

        # Decoder layers (for cross-attention to observations)
        self.decoder_layers = nn.ModuleList([
            nn.TransformerDecoderLayer(
                d_model=d_model,
                nhead=num_heads,
                dim_feedforward=dim_feedforward,
                dropout=dropout,
                batch_first=True,
            )
            for _ in range(num_layers)
        ])

        self.norm = nn.LayerNorm(d_model)

    def forward(
        self,
        action_sequence: torch.Tensor,    # (B, T, d_model)
        observation_features: torch.Tensor,  # (B, S, d_model)
    ) -> torch.Tensor:
        """
        Returns: Processed action features (B, T, d_model)
        """
        B, T, _ = action_sequence.shape

        # Self-attention with causal mask
        mask = None
        if self.use_causal_mask:
            mask = self._generate_causal_mask(T, action_sequence.device)

        # Encoder: self-attention on actions
        x = action_sequence
        for layer in self.encoder_layers:
            x = layer(x, src_mask=mask)

        # Decoder: cross-attention to observations
        for layer in self.decoder_layers:
            x = layer(x, observation_features, tgt_mask=mask)

        return self.norm(x)

    def _generate_causal_mask(self, size: int, device: torch.device) -> torch.Tensor:
        mask = torch.triu(torch.ones(size, size, device=device), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask
```

---

## Stage 1: Temporal Encoder Training

### Temporal Encoder Pretraining

```python
from train.world_model import TemporalEncoderTrainer

class TemporalEncoderTrainer:
    """
    Pretrain temporal encoder on sequence prediction tasks.

    Tasks:
    1. Next observation prediction
    2. Action sequence prediction
    3. Temporal contrastive learning
    4. Masked sequence modeling
    """

    def __init__(
        self,
        encoder: HistoryEncoder,
        config: TemporalTrainingConfig,
    ):
        self.encoder = encoder
        self.config = config

    def train_next_observation_prediction(
        self,
        dataset: SequenceDataset,
        num_epochs: int = 100,
    ):
        """
        Train encoder to predict next observation from history.
        """
        # Prediction head
        predictor = nn.Sequential(
            nn.Linear(self.encoder.context_dim, 512),
            nn.ReLU(),
            nn.Linear(512, self.encoder.obs_dim),
        )

        optimizer = torch.optim.AdamW(
            list(self.encoder.parameters()) + list(predictor.parameters()),
            lr=self.config.learning_rate,
        )

        for epoch in range(num_epochs):
            for batch in dataset:
                obs_seq = batch["observations"]  # (B, T+1, obs_dim)
                action_seq = batch["actions"]    # (B, T, action_dim)

                # Split into history and target
                obs_history = obs_seq[:, :-1]    # (B, T, obs_dim)
                target_obs = obs_seq[:, -1]      # (B, obs_dim)

                # Encode history
                context = self.encoder(obs_history, action_seq)

                # Predict next observation
                predicted_obs = predictor(context[:, -1])

                # Loss
                loss = F.mse_loss(predicted_obs, target_obs)

                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

            print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

    def train_temporal_contrastive(
        self,
        dataset: SequenceDataset,
        num_epochs: int = 100,
    ):
        """
        Temporal contrastive learning (TCL).

        Positive pairs: Consecutive states in same trajectory
        Negative pairs: States from different trajectories
        """
        optimizer = torch.optim.AdamW(
            self.encoder.parameters(),
            lr=self.config.learning_rate,
        )

        for epoch in range(num_epochs):
            for batch in dataset:
                obs_seq = batch["observations"]
                action_seq = batch["actions"]

                B, T, _ = obs_seq.shape

                # Encode full sequence
                context = self.encoder(obs_seq, action_seq)  # (B, T, context_dim)

                # Positive pairs: consecutive steps
                anchor = context[:, :-1].reshape(-1, context.shape[-1])  # (B*(T-1), dim)
                positive = context[:, 1:].reshape(-1, context.shape[-1])  # (B*(T-1), dim)

                # Negative pairs: random sampling from batch
                perm = torch.randperm(anchor.shape[0])
                negative = anchor[perm]

                # InfoNCE loss
                similarity_pos = F.cosine_similarity(anchor, positive)
                similarity_neg = F.cosine_similarity(anchor, negative)

                # Temperature-scaled softmax
                temperature = 0.07
                logits = torch.cat([
                    similarity_pos.unsqueeze(-1),
                    similarity_neg.unsqueeze(-1),
                ], dim=-1) / temperature

                labels = torch.zeros(logits.shape[0], dtype=torch.long, device=logits.device)
                loss = F.cross_entropy(logits, labels)

                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

            print(f"Epoch {epoch}: TCL Loss = {loss.item():.4f}")

    def train_masked_sequence_modeling(
        self,
        dataset: SequenceDataset,
        mask_ratio: float = 0.15,
        num_epochs: int = 100,
    ):
        """
        Masked sequence modeling (MSM).

        Randomly mask observations/actions and predict them.
        """
        # Mask token
        mask_token = nn.Parameter(torch.zeros(self.encoder.context_dim))

        # Reconstruction heads
        obs_decoder = nn.Linear(self.encoder.context_dim, self.encoder.obs_dim)
        action_decoder = nn.Linear(self.encoder.context_dim, self.encoder.action_dim)

        optimizer = torch.optim.AdamW(
            list(self.encoder.parameters()) +
            [mask_token] +
            list(obs_decoder.parameters()) +
            list(action_decoder.parameters()),
            lr=self.config.learning_rate,
        )

        for epoch in range(num_epochs):
            for batch in dataset:
                obs_seq = batch["observations"]
                action_seq = batch["actions"]

                B, T, obs_dim = obs_seq.shape

                # Generate mask
                obs_mask = torch.rand(B, T) < mask_ratio
                action_mask = torch.rand(B, T) < mask_ratio

                # Apply mask
                masked_obs = obs_seq.clone()
                masked_obs[obs_mask] = 0  # Zero out masked positions

                masked_actions = action_seq.clone()
                masked_actions[action_mask] = 0

                # Encode masked sequence
                context = self.encoder(masked_obs, masked_actions)

                # Reconstruct masked positions
                obs_pred = obs_decoder(context)
                action_pred = action_decoder(context)

                # Loss only on masked positions
                obs_loss = F.mse_loss(
                    obs_pred[obs_mask],
                    obs_seq[obs_mask],
                )
                action_loss = F.mse_loss(
                    action_pred[action_mask],
                    action_seq[action_mask],
                )

                loss = obs_loss + action_loss

                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

            print(f"Epoch {epoch}: MSM Loss = {loss.item():.4f}")
```

---

## Stage 2: World Model Training

### RSSM (Recurrent State-Space Model)

```python
class RSSM(nn.Module):
    """
    Recurrent State-Space Model for world modeling.

    Reference: "Dream to Control: Learning Behaviors by Latent Imagination"
               (Hafner et al., Dreamer series)

    State = (deterministic_state, stochastic_state)
    - Deterministic: GRU hidden state
    - Stochastic: Sampled from learned distribution

    Components:
    1. Encoder: obs -> posterior (z|obs, h)
    2. Prior: action -> prior (z|h)
    3. Dynamics: (z, a) -> next_h
    4. Decoder: (z, h) -> obs_reconstruction
    """

    def __init__(
        self,
        obs_dim: int,
        action_dim: int,
        state_dim: int = 256,
        stochastic_dim: int = 32,
        hidden_dim: int = 512,
        num_layers: int = 2,
        use_discrete: bool = False,
        num_categories: int = 32,
        num_classes: int = 32,
    ):
        super().__init__()
        self.state_dim = state_dim
        self.stochastic_dim = stochastic_dim
        self.hidden_dim = hidden_dim
        self.use_discrete = use_discrete

        # Deterministic state transition (GRU)
        self.gru = nn.GRU(
            input_size=stochastic_dim + action_dim,
            hidden_size=state_dim,
            num_layers=num_layers,
            batch_first=True,
        )

        # Prior network: h -> prior(z)
        self.prior_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ELU(),
        )

        # Posterior network: (h, obs) -> posterior(z)
        self.posterior_net = nn.Sequential(
            nn.Linear(state_dim + obs_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ELU(),
        )

        if use_discrete:
            # DreamerV3-style discrete latent
            self.num_categories = num_categories
            self.num_classes = num_classes
            self.stochastic_dim = num_categories * num_classes

            self.prior_logits = nn.Linear(hidden_dim, num_categories * num_classes)
            self.posterior_logits = nn.Linear(hidden_dim, num_categories * num_classes)
        else:
            # Gaussian latent
            self.prior_mean = nn.Linear(hidden_dim, stochastic_dim)
            self.prior_std = nn.Linear(hidden_dim, stochastic_dim)
            self.posterior_mean = nn.Linear(hidden_dim, stochastic_dim)
            self.posterior_std = nn.Linear(hidden_dim, stochastic_dim)

    def initial_state(self, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get initial state."""
        state = torch.zeros(batch_size, self.state_dim, device=device)
        stochastic = torch.zeros(batch_size, self.stochastic_dim, device=device)
        return state, stochastic

    def encode_sequence(
        self,
        observations: torch.Tensor,  # (B, T, obs_dim)
        actions: torch.Tensor,       # (B, T, action_dim)
    ) -> Dict[str, torch.Tensor]:
        """
        Encode observation sequence into latent states.

        Returns:
            - prior_state: Prior latent states
            - posterior_state: Posterior latent states (conditioned on obs)
            - prior_stochastic: Prior stochastic samples
            - posterior_stochastic: Posterior stochastic samples
            - kl_loss: KL divergence between prior and posterior
        """
        B, T, _ = observations.shape
        device = observations.device

        # Initialize
        state, stochastic = self.initial_state(B, device)

        prior_states = []
        posterior_states = []
        prior_stochastics = []
        posterior_stochastics = []
        kl_losses = []

        for t in range(T):
            # Get observation and action
            obs = observations[:, t]
            action = actions[:, t] if t < actions.shape[1] else torch.zeros(B, actions.shape[-1], device=device)

            # Prior: predict stochastic from deterministic state only
            prior_feat = self.prior_net(state)
            if self.use_discrete:
                prior_logits = self.prior_logits(prior_feat).view(B, self.num_categories, self.num_classes)
                prior_dist = torch.distributions.OneHotCategorical(logits=prior_logits)
                prior_sample = prior_dist.sample().view(B, -1)
            else:
                prior_mean = self.prior_mean(prior_feat)
                prior_std = F.softplus(self.prior_std(prior_feat)) + 0.1
                prior_dist = torch.distributions.Normal(prior_mean, prior_std)
                prior_sample = prior_dist.rsample()

            # Posterior: predict stochastic from deterministic state + observation
            posterior_feat = self.posterior_net(torch.cat([state, obs], dim=-1))
            if self.use_discrete:
                posterior_logits = self.posterior_logits(posterior_feat).view(B, self.num_categories, self.num_classes)
                posterior_dist = torch.distributions.OneHotCategorical(logits=posterior_logits)
                posterior_sample = posterior_dist.sample() + posterior_dist.probs - posterior_dist.probs.detach()
                posterior_sample = posterior_sample.view(B, -1)
            else:
                posterior_mean = self.posterior_mean(posterior_feat)
                posterior_std = F.softplus(self.posterior_std(posterior_feat)) + 0.1
                posterior_dist = torch.distributions.Normal(posterior_mean, posterior_std)
                posterior_sample = posterior_dist.rsample()

            # KL divergence
            if self.use_discrete:
                kl = torch.distributions.kl_divergence(posterior_dist, prior_dist).sum(dim=-1)
            else:
                kl = torch.distributions.kl_divergence(posterior_dist, prior_dist).sum(dim=-1)

            # Update deterministic state
            gru_input = torch.cat([posterior_sample, action], dim=-1).unsqueeze(1)
            _, state = self.gru(gru_input, state.unsqueeze(0))
            state = state.squeeze(0)

            # Store
            prior_states.append(state)
            posterior_states.append(state)
            prior_stochastics.append(prior_sample)
            posterior_stochastics.append(posterior_sample)
            kl_losses.append(kl)

        return {
            "prior_state": torch.stack(prior_states, dim=1),
            "posterior_state": torch.stack(posterior_states, dim=1),
            "prior_stochastic": torch.stack(prior_stochastics, dim=1),
            "posterior_stochastic": torch.stack(posterior_stochastics, dim=1),
            "kl_loss": torch.stack(kl_losses, dim=1),
        }

    def imagine_step(
        self,
        state: torch.Tensor,         # (B, state_dim)
        stochastic: torch.Tensor,    # (B, stochastic_dim)
        action: torch.Tensor,        # (B, action_dim)
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Imagine one step forward using prior (without observation).

        Returns:
            - next_state: Next deterministic state
            - next_stochastic: Next stochastic sample from prior
        """
        # Update deterministic state
        gru_input = torch.cat([stochastic, action], dim=-1).unsqueeze(1)
        _, next_state = self.gru(gru_input, state.unsqueeze(0))
        next_state = next_state.squeeze(0)

        # Sample from prior
        prior_feat = self.prior_net(next_state)
        if self.use_discrete:
            prior_logits = self.prior_logits(prior_feat).view(-1, self.num_categories, self.num_classes)
            prior_dist = torch.distributions.OneHotCategorical(logits=prior_logits)
            next_stochastic = prior_dist.sample().view(-1, self.stochastic_dim)
        else:
            prior_mean = self.prior_mean(prior_feat)
            prior_std = F.softplus(self.prior_std(prior_feat)) + 0.1
            next_stochastic = prior_mean + torch.randn_like(prior_std) * prior_std

        return next_state, next_stochastic


class WorldModelTrainer:
    """
    Train RSSM world model.
    """

    def __init__(
        self,
        world_model: RSSM,
        obs_decoder: nn.Module,
        reward_predictor: nn.Module,
        continue_predictor: nn.Module,
        config: WorldModelTrainingConfig,
    ):
        self.world_model = world_model
        self.obs_decoder = obs_decoder
        self.reward_predictor = reward_predictor
        self.continue_predictor = continue_predictor
        self.config = config

    def train(
        self,
        dataset: TrajectoryDataset,
        num_epochs: int = 100,
    ):
        """
        Train world model with reconstruction + KL objectives.
        """
        optimizer = torch.optim.AdamW(
            list(self.world_model.parameters()) +
            list(self.obs_decoder.parameters()) +
            list(self.reward_predictor.parameters()) +
            list(self.continue_predictor.parameters()),
            lr=self.config.learning_rate,
        )

        for epoch in range(num_epochs):
            epoch_losses = {"total": 0, "recon": 0, "kl": 0, "reward": 0, "continue": 0}

            for batch in dataset:
                observations = batch["observations"]
                actions = batch["actions"]
                rewards = batch["rewards"]
                continues = batch["continues"]  # 1.0 if not terminal, 0.0 if terminal

                # Encode sequence
                latent = self.world_model.encode_sequence(observations, actions)

                # Full state (deterministic + stochastic)
                full_state = torch.cat([
                    latent["posterior_state"],
                    latent["posterior_stochastic"],
                ], dim=-1)

                # Reconstruction loss
                obs_recon = self.obs_decoder(full_state)
                recon_loss = F.mse_loss(obs_recon, observations)

                # KL loss with free bits
                kl_loss = latent["kl_loss"].mean()
                kl_loss = torch.clamp(kl_loss, min=self.config.free_nats)

                # Reward prediction loss
                reward_pred = self.reward_predictor(full_state)
                reward_loss = F.mse_loss(reward_pred.squeeze(-1), rewards)

                # Continue prediction loss (binary)
                continue_pred = self.continue_predictor(full_state)
                continue_loss = F.binary_cross_entropy_with_logits(
                    continue_pred.squeeze(-1),
                    continues,
                )

                # Total loss
                total_loss = (
                    self.config.reconstruction_weight * recon_loss +
                    self.config.kl_weight * kl_loss +
                    self.config.reward_weight * reward_loss +
                    self.config.continue_weight * continue_loss
                )

                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    list(self.world_model.parameters()) +
                    list(self.obs_decoder.parameters()) +
                    list(self.reward_predictor.parameters()) +
                    list(self.continue_predictor.parameters()),
                    max_norm=100.0,
                )
                optimizer.step()
                optimizer.zero_grad()

                # Track losses
                epoch_losses["total"] += total_loss.item()
                epoch_losses["recon"] += recon_loss.item()
                epoch_losses["kl"] += kl_loss.item()
                epoch_losses["reward"] += reward_loss.item()
                epoch_losses["continue"] += continue_loss.item()

            # Print epoch summary
            n_batches = len(dataset)
            print(f"Epoch {epoch}:")
            print(f"  Total: {epoch_losses['total']/n_batches:.4f}")
            print(f"  Recon: {epoch_losses['recon']/n_batches:.4f}")
            print(f"  KL: {epoch_losses['kl']/n_batches:.4f}")
            print(f"  Reward: {epoch_losses['reward']/n_batches:.4f}")
            print(f"  Continue: {epoch_losses['continue']/n_batches:.4f}")
```

---

## Stage 3: Latent Dynamics Learning

### Latent Dynamics Models

```python
class LatentDynamicsModel(nn.Module):
    """
    Learn dynamics in latent space.

    Types:
    1. Deterministic: MLP/GRU for mean prediction
    2. Stochastic: Gaussian/Categorical distribution
    3. Ensemble: Multiple models for uncertainty
    4. Flow-based: Normalizing flows for complex distributions
    """

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dim: int = 512,
        dynamics_type: str = "stochastic",
        ensemble_size: int = 5,
    ):
        super().__init__()
        self.dynamics_type = dynamics_type

        if dynamics_type == "deterministic":
            self.model = nn.Sequential(
                nn.Linear(state_dim + action_dim, hidden_dim),
                nn.ELU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ELU(),
                nn.Linear(hidden_dim, state_dim),
            )

        elif dynamics_type == "stochastic":
            self.base = nn.Sequential(
                nn.Linear(state_dim + action_dim, hidden_dim),
                nn.ELU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ELU(),
            )
            self.mean = nn.Linear(hidden_dim, state_dim)
            self.log_std = nn.Linear(hidden_dim, state_dim)

        elif dynamics_type == "ensemble":
            self.models = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(state_dim + action_dim, hidden_dim),
                    nn.ELU(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ELU(),
                    nn.Linear(hidden_dim, state_dim),
                )
                for _ in range(ensemble_size)
            ])

        elif dynamics_type == "flow":
            self.flow = RealNVP(
                input_dim=state_dim,
                condition_dim=state_dim + action_dim,
                hidden_dim=hidden_dim,
                num_blocks=4,
            )

    def forward(
        self,
        state: torch.Tensor,
        action: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """
        Predict next state distribution.

        Returns:
            - mean: Mean of next state
            - std: Standard deviation (if stochastic)
            - sample: Sampled next state
            - uncertainty: Epistemic uncertainty (if ensemble)
        """
        x = torch.cat([state, action], dim=-1)

        if self.dynamics_type == "deterministic":
            mean = self.model(x)
            return {"mean": mean, "sample": mean}

        elif self.dynamics_type == "stochastic":
            h = self.base(x)
            mean = self.mean(h)
            log_std = self.log_std(h).clamp(-10, 2)
            std = torch.exp(log_std)
            sample = mean + torch.randn_like(std) * std
            return {"mean": mean, "std": std, "sample": sample}

        elif self.dynamics_type == "ensemble":
            predictions = [model(x) for model in self.models]
            predictions = torch.stack(predictions, dim=0)  # (E, B, state_dim)
            mean = predictions.mean(dim=0)
            std = predictions.std(dim=0)
            uncertainty = std.mean(dim=-1)  # Epistemic uncertainty

            # Sample from random ensemble member
            idx = torch.randint(0, len(self.models), (1,)).item()
            sample = predictions[idx]

            return {
                "mean": mean,
                "std": std,
                "sample": sample,
                "uncertainty": uncertainty,
            }

        elif self.dynamics_type == "flow":
            sample, log_prob = self.flow.sample(x)
            return {"sample": sample, "log_prob": log_prob}


class LatentDynamicsTrainer:
    """
    Train latent dynamics model.
    """

    def __init__(
        self,
        dynamics_model: LatentDynamicsModel,
        encoder: nn.Module,
        config: DynamicsTrainingConfig,
    ):
        self.dynamics_model = dynamics_model
        self.encoder = encoder
        self.config = config

    def train(
        self,
        dataset: TrajectoryDataset,
        num_epochs: int = 100,
    ):
        """Train dynamics model."""
        optimizer = torch.optim.AdamW(
            self.dynamics_model.parameters(),
            lr=self.config.learning_rate,
        )

        for epoch in range(num_epochs):
            for batch in dataset:
                observations = batch["observations"]
                actions = batch["actions"]

                # Encode to latent space
                with torch.no_grad():
                    latent_states = self.encoder(observations)

                # Prepare training data
                current_states = latent_states[:, :-1]
                next_states = latent_states[:, 1:]
                actions = actions[:, :-1]

                # Flatten batch and time
                B, T, D = current_states.shape
                current_states = current_states.reshape(B * T, D)
                next_states = next_states.reshape(B * T, D)
                actions = actions.reshape(B * T, -1)

                # Predict next state
                output = self.dynamics_model(current_states, actions)

                # Loss
                if self.dynamics_model.dynamics_type == "stochastic":
                    # Negative log likelihood
                    dist = torch.distributions.Normal(output["mean"], output["std"])
                    loss = -dist.log_prob(next_states).mean()
                else:
                    # MSE loss
                    loss = F.mse_loss(output["mean"], next_states)

                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

            print(f"Epoch {epoch}: Dynamics Loss = {loss.item():.4f}")
```

---

## Stage 4: Imagination-Based Planning

### Cross-Entropy Method (CEM) Planning

```python
class CEMPlanner:
    """
    Planning with Cross-Entropy Method in latent space.

    Algorithm:
    1. Sample action sequences from distribution
    2. Evaluate using world model imagination
    3. Select top-k sequences
    4. Update distribution to top-k mean/std
    5. Repeat for N iterations
    """

    def __init__(
        self,
        world_model: RSSM,
        reward_predictor: nn.Module,
        action_dim: int,
        planning_horizon: int = 15,
        num_samples: int = 1000,
        top_k: int = 100,
        num_iterations: int = 5,
    ):
        self.world_model = world_model
        self.reward_predictor = reward_predictor
        self.action_dim = action_dim
        self.planning_horizon = planning_horizon
        self.num_samples = num_samples
        self.top_k = top_k
        self.num_iterations = num_iterations

    def plan(
        self,
        initial_state: torch.Tensor,
        initial_stochastic: torch.Tensor,
    ) -> torch.Tensor:
        """
        Plan optimal action sequence from current state.

        Returns: Best action for current timestep
        """
        device = initial_state.device
        B = initial_state.shape[0]

        # Initialize action distribution
        action_mean = torch.zeros(self.planning_horizon, self.action_dim, device=device)
        action_std = torch.ones(self.planning_horizon, self.action_dim, device=device)

        for iteration in range(self.num_iterations):
            # Sample action sequences
            # (num_samples, planning_horizon, action_dim)
            action_samples = (
                action_mean.unsqueeze(0) +
                torch.randn(self.num_samples, self.planning_horizon, self.action_dim, device=device) *
                action_std.unsqueeze(0)
            )

            # Clip to action bounds
            action_samples = torch.clamp(action_samples, -1.0, 1.0)

            # Evaluate each sequence with world model
            rewards = self._evaluate_sequences(
                initial_state.expand(self.num_samples, -1),
                initial_stochastic.expand(self.num_samples, -1),
                action_samples,
            )  # (num_samples,)

            # Select top-k
            top_indices = torch.topk(rewards, self.top_k).indices
            top_actions = action_samples[top_indices]  # (top_k, planning_horizon, action_dim)

            # Update distribution
            action_mean = top_actions.mean(dim=0)
            action_std = top_actions.std(dim=0) + 1e-6

        # Return first action of best sequence
        return action_mean[0]

    def _evaluate_sequences(
        self,
        state: torch.Tensor,        # (N, state_dim)
        stochastic: torch.Tensor,   # (N, stochastic_dim)
        actions: torch.Tensor,      # (N, H, action_dim)
    ) -> torch.Tensor:
        """
        Evaluate action sequences using world model imagination.

        Returns: Total expected reward for each sequence
        """
        N = state.shape[0]
        total_rewards = torch.zeros(N, device=state.device)

        current_state = state
        current_stochastic = stochastic
        gamma = 0.99

        for t in range(self.planning_horizon):
            # Imagine one step
            next_state, next_stochastic = self.world_model.imagine_step(
                current_state,
                current_stochastic,
                actions[:, t],
            )

            # Predict reward
            full_state = torch.cat([next_state, next_stochastic], dim=-1)
            reward = self.reward_predictor(full_state).squeeze(-1)

            # Accumulate discounted reward
            total_rewards += (gamma ** t) * reward

            # Update state
            current_state = next_state
            current_stochastic = next_stochastic

        return total_rewards


class MPPIPlanner:
    """
    Model Predictive Path Integral (MPPI) Planning.

    More sophisticated than CEM:
    - Importance sampling with temperature
    - Smooth trajectory optimization
    """

    def __init__(
        self,
        world_model: RSSM,
        reward_predictor: nn.Module,
        action_dim: int,
        planning_horizon: int = 15,
        num_samples: int = 1000,
        temperature: float = 0.1,
        noise_std: float = 0.5,
    ):
        self.world_model = world_model
        self.reward_predictor = reward_predictor
        self.action_dim = action_dim
        self.planning_horizon = planning_horizon
        self.num_samples = num_samples
        self.temperature = temperature
        self.noise_std = noise_std

        # Previous solution (warm start)
        self.prev_actions = None

    def plan(
        self,
        initial_state: torch.Tensor,
        initial_stochastic: torch.Tensor,
    ) -> torch.Tensor:
        """Plan with MPPI."""
        device = initial_state.device

        # Warm start from previous solution
        if self.prev_actions is None:
            mean_actions = torch.zeros(
                self.planning_horizon, self.action_dim, device=device
            )
        else:
            # Shift previous solution
            mean_actions = torch.cat([
                self.prev_actions[1:],
                self.prev_actions[-1:],
            ], dim=0)

        # Sample action perturbations
        noise = torch.randn(
            self.num_samples, self.planning_horizon, self.action_dim, device=device
        ) * self.noise_std

        action_samples = mean_actions.unsqueeze(0) + noise
        action_samples = torch.clamp(action_samples, -1.0, 1.0)

        # Evaluate sequences
        costs = -self._evaluate_sequences(
            initial_state.expand(self.num_samples, -1),
            initial_stochastic.expand(self.num_samples, -1),
            action_samples,
        )  # Negative reward = cost

        # MPPI weighting
        weights = F.softmax(-costs / self.temperature, dim=0)  # (num_samples,)

        # Weighted average of actions
        optimal_actions = (weights.unsqueeze(-1).unsqueeze(-1) * action_samples).sum(dim=0)

        # Store for warm start
        self.prev_actions = optimal_actions

        return optimal_actions[0]

    def _evaluate_sequences(
        self,
        state: torch.Tensor,
        stochastic: torch.Tensor,
        actions: torch.Tensor,
    ) -> torch.Tensor:
        """Evaluate action sequences."""
        N = state.shape[0]
        total_rewards = torch.zeros(N, device=state.device)

        current_state = state
        current_stochastic = stochastic
        gamma = 0.99

        for t in range(self.planning_horizon):
            next_state, next_stochastic = self.world_model.imagine_step(
                current_state,
                current_stochastic,
                actions[:, t],
            )

            full_state = torch.cat([next_state, next_stochastic], dim=-1)
            reward = self.reward_predictor(full_state).squeeze(-1)

            total_rewards += (gamma ** t) * reward

            current_state = next_state
            current_stochastic = next_stochastic

        return total_rewards
```

---

## Stage 5: Model Predictive Control

### MPC with World Model

```python
class WorldModelMPC:
    """
    Model Predictive Control using learned world model.

    At each timestep:
    1. Encode current observation
    2. Plan optimal trajectory
    3. Execute first action
    4. Re-plan at next timestep
    """

    def __init__(
        self,
        world_model: RSSM,
        obs_encoder: nn.Module,
        reward_predictor: nn.Module,
        planner_type: str = "cem",
        planning_horizon: int = 15,
        replan_frequency: int = 1,
    ):
        self.world_model = world_model
        self.obs_encoder = obs_encoder
        self.reward_predictor = reward_predictor
        self.planning_horizon = planning_horizon
        self.replan_frequency = replan_frequency

        # Create planner
        if planner_type == "cem":
            self.planner = CEMPlanner(
                world_model=world_model,
                reward_predictor=reward_predictor,
                action_dim=7,
                planning_horizon=planning_horizon,
            )
        elif planner_type == "mppi":
            self.planner = MPPIPlanner(
                world_model=world_model,
                reward_predictor=reward_predictor,
                action_dim=7,
                planning_horizon=planning_horizon,
            )

        # State tracking
        self.current_state = None
        self.current_stochastic = None
        self.step_count = 0
        self.planned_actions = None

    def reset(self):
        """Reset MPC state."""
        self.current_state = None
        self.current_stochastic = None
        self.step_count = 0
        self.planned_actions = None

    def get_action(
        self,
        observation: torch.Tensor,
    ) -> torch.Tensor:
        """
        Get action for current observation.

        Args:
            observation: Current observation (B, obs_dim)

        Returns:
            action: Action to execute (B, action_dim)
        """
        B = observation.shape[0]
        device = observation.device

        # Encode observation
        with torch.no_grad():
            obs_features = self.obs_encoder(observation)

        # Initialize or update state
        if self.current_state is None:
            self.current_state, self.current_stochastic = self.world_model.initial_state(B, device)

        # Update posterior state with observation
        latent = self.world_model.encode_sequence(
            obs_features.unsqueeze(1),
            torch.zeros(B, 1, 7, device=device),  # Dummy action for single step
        )
        self.current_state = latent["posterior_state"][:, -1]
        self.current_stochastic = latent["posterior_stochastic"][:, -1]

        # Re-plan if needed
        if self.step_count % self.replan_frequency == 0 or self.planned_actions is None:
            with torch.no_grad():
                action = self.planner.plan(
                    self.current_state,
                    self.current_stochastic,
                )
            self.planned_actions = action  # Store planned action
        else:
            action = self.planned_actions

        self.step_count += 1

        return action


class RecedingHorizonMPC:
    """
    Receding Horizon MPC with trajectory tracking.

    Features:
    - Reference trajectory tracking
    - Constraint handling
    - Warm starting from previous solution
    """

    def __init__(
        self,
        world_model: RSSM,
        obs_encoder: nn.Module,
        planning_horizon: int = 20,
        control_horizon: int = 5,
    ):
        self.world_model = world_model
        self.obs_encoder = obs_encoder
        self.planning_horizon = planning_horizon
        self.control_horizon = control_horizon

        # Optimizer for trajectory optimization
        self.action_sequence = None

    def optimize_trajectory(
        self,
        initial_state: torch.Tensor,
        initial_stochastic: torch.Tensor,
        reference_trajectory: torch.Tensor,
        num_iterations: int = 50,
    ) -> torch.Tensor:
        """
        Optimize action sequence to track reference trajectory.

        Args:
            initial_state: Current latent state
            initial_stochastic: Current stochastic state
            reference_trajectory: Target trajectory to follow
            num_iterations: Optimization iterations

        Returns:
            Optimized action sequence
        """
        device = initial_state.device

        # Initialize action sequence
        if self.action_sequence is None:
            self.action_sequence = torch.zeros(
                self.planning_horizon, 7, device=device, requires_grad=True
            )
        else:
            # Warm start: shift previous solution
            with torch.no_grad():
                self.action_sequence[:-1] = self.action_sequence[1:].clone()
                self.action_sequence[-1] = 0
            self.action_sequence.requires_grad = True

        optimizer = torch.optim.Adam([self.action_sequence], lr=0.1)

        for iteration in range(num_iterations):
            optimizer.zero_grad()

            # Rollout with world model
            current_state = initial_state
            current_stochastic = initial_stochastic

            trajectory_loss = 0.0
            action_cost = 0.0

            for t in range(self.planning_horizon):
                # Imagine next state
                next_state, next_stochastic = self.world_model.imagine_step(
                    current_state,
                    current_stochastic,
                    self.action_sequence[t],
                )

                # Trajectory tracking cost
                if t < reference_trajectory.shape[0]:
                    trajectory_loss += F.mse_loss(next_state, reference_trajectory[t])

                # Action smoothness cost
                if t > 0:
                    action_cost += F.mse_loss(
                        self.action_sequence[t],
                        self.action_sequence[t-1],
                    )

                current_state = next_state
                current_stochastic = next_stochastic

            # Total cost
            total_cost = trajectory_loss + 0.1 * action_cost

            total_cost.backward()
            optimizer.step()

            # Clip actions
            with torch.no_grad():
                self.action_sequence.clamp_(-1.0, 1.0)

        return self.action_sequence[:self.control_horizon].detach()
```

---

## Stage 6: Policy Learning with World Models

### Dreamer-Style Actor-Critic

```python
class DreamerTrainer:
    """
    Dreamer: Policy learning through world model imagination.

    Reference: "Dream to Control: Learning Behaviors by Latent Imagination"

    Training:
    1. Collect real trajectories
    2. Train world model on real data
    3. Imagine trajectories using world model
    4. Train actor-critic on imagined trajectories
    """

    def __init__(
        self,
        world_model: RSSM,
        obs_encoder: nn.Module,
        obs_decoder: nn.Module,
        reward_predictor: nn.Module,
        continue_predictor: nn.Module,
        actor: nn.Module,
        critic: nn.Module,
        config: DreamerConfig,
    ):
        self.world_model = world_model
        self.obs_encoder = obs_encoder
        self.obs_decoder = obs_decoder
        self.reward_predictor = reward_predictor
        self.continue_predictor = continue_predictor
        self.actor = actor
        self.critic = critic
        self.config = config

        # Target critic for stability
        self.target_critic = copy.deepcopy(critic)

        # Optimizers
        self.world_model_optim = torch.optim.AdamW(
            list(world_model.parameters()) +
            list(obs_encoder.parameters()) +
            list(obs_decoder.parameters()) +
            list(reward_predictor.parameters()) +
            list(continue_predictor.parameters()),
            lr=config.world_model_lr,
        )

        self.actor_optim = torch.optim.AdamW(
            actor.parameters(),
            lr=config.actor_lr,
        )

        self.critic_optim = torch.optim.AdamW(
            critic.parameters(),
            lr=config.critic_lr,
        )

    def train_world_model(
        self,
        batch: Dict[str, torch.Tensor],
    ) -> Dict[str, float]:
        """Train world model on real data."""
        observations = batch["observations"]
        actions = batch["actions"]
        rewards = batch["rewards"]
        continues = batch["continues"]

        # Encode observations
        obs_features = self.obs_encoder(observations)

        # Run world model
        latent = self.world_model.encode_sequence(obs_features, actions)

        # Full state
        full_state = torch.cat([
            latent["posterior_state"],
            latent["posterior_stochastic"],
        ], dim=-1)

        # Reconstruction loss
        obs_recon = self.obs_decoder(full_state)
        recon_loss = F.mse_loss(obs_recon, obs_features)

        # KL loss with free bits
        kl_loss = latent["kl_loss"].mean()
        kl_loss = torch.clamp(kl_loss - self.config.free_nats, min=0.0)

        # Reward prediction
        reward_pred = self.reward_predictor(full_state)
        reward_loss = F.mse_loss(reward_pred.squeeze(-1), rewards)

        # Continue prediction
        continue_pred = self.continue_predictor(full_state)
        continue_loss = F.binary_cross_entropy_with_logits(
            continue_pred.squeeze(-1),
            continues,
        )

        # Total loss
        total_loss = (
            self.config.recon_weight * recon_loss +
            self.config.kl_weight * kl_loss +
            self.config.reward_weight * reward_loss +
            self.config.continue_weight * continue_loss
        )

        self.world_model_optim.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(
            list(self.world_model.parameters()) +
            list(self.obs_encoder.parameters()) +
            list(self.obs_decoder.parameters()),
            max_norm=100.0,
        )
        self.world_model_optim.step()

        return {
            "world_model/total_loss": total_loss.item(),
            "world_model/recon_loss": recon_loss.item(),
            "world_model/kl_loss": kl_loss.item(),
            "world_model/reward_loss": reward_loss.item(),
        }

    def train_actor_critic(
        self,
        initial_states: torch.Tensor,
        initial_stochastics: torch.Tensor,
    ) -> Dict[str, float]:
        """
        Train actor-critic on imagined trajectories.
        """
        # Imagine trajectories
        imagined = self._imagine_trajectories(
            initial_states,
            initial_stochastics,
            horizon=self.config.imagination_horizon,
        )

        states = imagined["states"]           # (B, H, state_dim)
        stochastics = imagined["stochastics"] # (B, H, stochastic_dim)
        rewards = imagined["rewards"]         # (B, H)
        continues = imagined["continues"]     # (B, H)
        actions = imagined["actions"]         # (B, H, action_dim)

        # Full states
        full_states = torch.cat([states, stochastics], dim=-1)

        # ====== Critic Training ======
        with torch.no_grad():
            # Compute returns (lambda-returns)
            returns = self._compute_lambda_returns(
                rewards,
                continues,
                full_states,
            )

        # Critic prediction
        value_pred = self.critic(full_states.detach())
        critic_loss = F.mse_loss(value_pred.squeeze(-1), returns)

        self.critic_optim.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=100.0)
        self.critic_optim.step()

        # ====== Actor Training ======
        # Re-imagine with actor gradient
        imagined_for_actor = self._imagine_trajectories(
            initial_states.detach(),
            initial_stochastics.detach(),
            horizon=self.config.imagination_horizon,
        )

        states_actor = imagined_for_actor["states"]
        stochastics_actor = imagined_for_actor["stochastics"]
        rewards_actor = imagined_for_actor["rewards"]
        continues_actor = imagined_for_actor["continues"]

        full_states_actor = torch.cat([states_actor, stochastics_actor], dim=-1)

        # Compute returns for actor
        with torch.no_grad():
            returns_actor = self._compute_lambda_returns(
                rewards_actor,
                continues_actor,
                full_states_actor,
            )

        # Actor loss: negative of returns
        actor_loss = -returns_actor.mean()

        # Entropy bonus
        action_mean, action_std = self.actor(full_states_actor)
        entropy = torch.distributions.Normal(action_mean, action_std).entropy().mean()
        actor_loss -= self.config.entropy_weight * entropy

        self.actor_optim.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=100.0)
        self.actor_optim.step()

        # Update target critic
        self._soft_update_target()

        return {
            "actor/loss": actor_loss.item(),
            "actor/entropy": entropy.item(),
            "critic/loss": critic_loss.item(),
            "returns/mean": returns.mean().item(),
        }

    def _imagine_trajectories(
        self,
        initial_state: torch.Tensor,
        initial_stochastic: torch.Tensor,
        horizon: int,
    ) -> Dict[str, torch.Tensor]:
        """Imagine trajectories using actor policy."""
        states = []
        stochastics = []
        rewards = []
        continues = []
        actions = []

        current_state = initial_state
        current_stochastic = initial_stochastic

        for t in range(horizon):
            full_state = torch.cat([current_state, current_stochastic], dim=-1)

            # Sample action from actor
            action_mean, action_std = self.actor(full_state)
            action = action_mean + torch.randn_like(action_std) * action_std

            # Imagine next state
            next_state, next_stochastic = self.world_model.imagine_step(
                current_state, current_stochastic, action
            )

            # Predict reward and continue
            next_full_state = torch.cat([next_state, next_stochastic], dim=-1)
            reward = self.reward_predictor(next_full_state).squeeze(-1)
            cont = torch.sigmoid(self.continue_predictor(next_full_state).squeeze(-1))

            # Store
            states.append(next_state)
            stochastics.append(next_stochastic)
            rewards.append(reward)
            continues.append(cont)
            actions.append(action)

            current_state = next_state
            current_stochastic = next_stochastic

        return {
            "states": torch.stack(states, dim=1),
            "stochastics": torch.stack(stochastics, dim=1),
            "rewards": torch.stack(rewards, dim=1),
            "continues": torch.stack(continues, dim=1),
            "actions": torch.stack(actions, dim=1),
        }

    def _compute_lambda_returns(
        self,
        rewards: torch.Tensor,      # (B, H)
        continues: torch.Tensor,    # (B, H)
        full_states: torch.Tensor,  # (B, H, state_dim)
        lambda_: float = 0.95,
    ) -> torch.Tensor:
        """Compute lambda-returns (TD(lambda))."""
        B, H, _ = full_states.shape

        # Value predictions
        values = self.target_critic(full_states).squeeze(-1)  # (B, H)

        returns = torch.zeros_like(rewards)
        last_return = values[:, -1]

        for t in reversed(range(H)):
            returns[:, t] = rewards[:, t] + self.config.gamma * continues[:, t] * (
                lambda_ * last_return + (1 - lambda_) * values[:, t]
            )
            last_return = returns[:, t]

        return returns

    def _soft_update_target(self, tau: float = 0.02):
        """Soft update target critic."""
        for param, target_param in zip(
            self.critic.parameters(),
            self.target_critic.parameters()
        ):
            target_param.data.copy_(
                tau * param.data + (1 - tau) * target_param.data
            )
```

---

## Advanced Topics

### Multi-Step Prediction with Uncertainty

```python
class UncertaintyAwareWorldModel:
    """
    World model with epistemic uncertainty estimation.

    Uses ensemble of models to quantify uncertainty.
    """

    def __init__(
        self,
        ensemble_size: int = 5,
        state_dim: int = 256,
        action_dim: int = 7,
    ):
        self.ensemble = nn.ModuleList([
            RSSM(
                obs_dim=512,
                action_dim=action_dim,
                state_dim=state_dim,
            )
            for _ in range(ensemble_size)
        ])

    def imagine_with_uncertainty(
        self,
        state: torch.Tensor,
        stochastic: torch.Tensor,
        action: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """
        Imagine with uncertainty quantification.

        Returns:
            - mean_state: Mean prediction
            - std_state: Prediction uncertainty
            - epistemic_uncertainty: Model disagreement
        """
        predictions = []

        for model in self.ensemble:
            next_state, next_stochastic = model.imagine_step(
                state, stochastic, action
            )
            predictions.append(next_state)

        predictions = torch.stack(predictions, dim=0)  # (E, B, state_dim)

        mean_state = predictions.mean(dim=0)
        std_state = predictions.std(dim=0)
        epistemic_uncertainty = std_state.mean(dim=-1)  # Scalar per sample

        return {
            "mean_state": mean_state,
            "std_state": std_state,
            "epistemic_uncertainty": epistemic_uncertainty,
        }


class SafePlanningWithUncertainty:
    """
    Plan while avoiding high-uncertainty regions.
    """

    def __init__(
        self,
        world_model: UncertaintyAwareWorldModel,
        uncertainty_threshold: float = 0.5,
    ):
        self.world_model = world_model
        self.uncertainty_threshold = uncertainty_threshold

    def plan_safe_trajectory(
        self,
        initial_state: torch.Tensor,
        initial_stochastic: torch.Tensor,
        horizon: int = 15,
    ) -> torch.Tensor:
        """
        Plan trajectory while penalizing high uncertainty.
        """
        # CEM with uncertainty penalty
        num_samples = 1000
        device = initial_state.device

        action_mean = torch.zeros(horizon, 7, device=device)
        action_std = torch.ones(horizon, 7, device=device)

        for iteration in range(5):
            action_samples = (
                action_mean.unsqueeze(0) +
                torch.randn(num_samples, horizon, 7, device=device) *
                action_std.unsqueeze(0)
            )

            # Evaluate with uncertainty
            total_costs = torch.zeros(num_samples, device=device)

            current_state = initial_state.expand(num_samples, -1)
            current_stochastic = initial_stochastic.expand(num_samples, -1)

            for t in range(horizon):
                output = self.world_model.imagine_with_uncertainty(
                    current_state,
                    current_stochastic,
                    action_samples[:, t],
                )

                # Reward (example: negative distance to goal)
                reward = -torch.norm(output["mean_state"] - self.goal_state, dim=-1)

                # Uncertainty penalty
                uncertainty_penalty = (
                    output["epistemic_uncertainty"] *
                    self.uncertainty_threshold
                )

                total_costs += -reward + uncertainty_penalty

                current_state = output["mean_state"]

            # Select top-k
            top_indices = torch.topk(-total_costs, 100).indices
            top_actions = action_samples[top_indices]

            action_mean = top_actions.mean(dim=0)
            action_std = top_actions.std(dim=0) + 1e-6

        return action_mean
```

### Hierarchical World Models

```python
class HierarchicalWorldModel:
    """
    Hierarchical world model with multiple temporal scales.

    Levels:
    1. Low-level: Fine-grained dynamics (e.g., 10Hz)
    2. Mid-level: Skill-level dynamics (e.g., 1Hz)
    3. High-level: Goal-level dynamics (e.g., 0.1Hz)
    """

    def __init__(
        self,
        low_level_model: RSSM,
        mid_level_model: RSSM,
        high_level_model: RSSM,
        low_to_mid_ratio: int = 10,
        mid_to_high_ratio: int = 10,
    ):
        self.low_level = low_level_model
        self.mid_level = mid_level_model
        self.high_level = high_level_model

        self.low_to_mid_ratio = low_to_mid_ratio
        self.mid_to_high_ratio = mid_to_high_ratio

        # Abstraction layers
        self.low_to_mid_abstractor = nn.Linear(
            low_level_model.state_dim,
            mid_level_model.state_dim,
        )
        self.mid_to_high_abstractor = nn.Linear(
            mid_level_model.state_dim,
            high_level_model.state_dim,
        )

    def plan_hierarchical(
        self,
        current_state: Dict[str, torch.Tensor],
        goal: torch.Tensor,
    ) -> torch.Tensor:
        """
        Hierarchical planning:
        1. High-level: Plan subgoal sequence
        2. Mid-level: Plan skill sequence to reach subgoals
        3. Low-level: Generate actions to execute skills
        """
        # High-level: Plan subgoal sequence
        high_state = current_state["high"]
        subgoals = self._plan_high_level(high_state, goal)

        # Mid-level: Plan skills to reach first subgoal
        mid_state = current_state["mid"]
        skills = self._plan_mid_level(mid_state, subgoals[0])

        # Low-level: Generate actions for first skill
        low_state = current_state["low"]
        actions = self._plan_low_level(low_state, skills[0])

        return actions

    def _plan_high_level(
        self,
        state: torch.Tensor,
        goal: torch.Tensor,
    ) -> torch.Tensor:
        """Plan subgoal sequence at high level."""
        # Use high-level world model to plan
        planner = CEMPlanner(
            world_model=self.high_level,
            reward_predictor=lambda s: -torch.norm(s - goal, dim=-1),
            action_dim=self.high_level.state_dim,  # Subgoal = latent state
            planning_horizon=5,
        )
        return planner.plan(state, state)

    def _plan_mid_level(
        self,
        state: torch.Tensor,
        subgoal: torch.Tensor,
    ) -> torch.Tensor:
        """Plan skill sequence to reach subgoal."""
        planner = CEMPlanner(
            world_model=self.mid_level,
            reward_predictor=lambda s: -torch.norm(s - subgoal, dim=-1),
            action_dim=self.mid_level.state_dim,
            planning_horizon=10,
        )
        return planner.plan(state, state)

    def _plan_low_level(
        self,
        state: torch.Tensor,
        skill: torch.Tensor,
    ) -> torch.Tensor:
        """Generate actions for skill execution."""
        planner = CEMPlanner(
            world_model=self.low_level,
            reward_predictor=lambda s: -torch.norm(s - skill, dim=-1),
            action_dim=7,
            planning_horizon=10,
        )
        return planner.plan(state, state)
```

---

## Deployment

### Real-Time World Model Inference

```python
class RealTimeWorldModelController:
    """
    Deploy world model for real-time control.
    """

    def __init__(
        self,
        model_path: str,
        planning_type: str = "mpc",
        control_frequency: float = 10.0,
    ):
        # Load models
        self.world_model = RSSM.from_pretrained(f"{model_path}/world_model.pt")
        self.obs_encoder = VisionEncoder.from_pretrained(f"{model_path}/encoder.pt")
        self.reward_predictor = RewardPredictor.from_pretrained(f"{model_path}/reward.pt")

        # Optimize for inference
        self.world_model = torch.jit.script(self.world_model)
        self.obs_encoder = torch.jit.script(self.obs_encoder)

        # Create controller
        if planning_type == "mpc":
            self.controller = WorldModelMPC(
                world_model=self.world_model,
                obs_encoder=self.obs_encoder,
                reward_predictor=self.reward_predictor,
            )
        elif planning_type == "actor":
            self.actor = Actor.from_pretrained(f"{model_path}/actor.pt")
            self.controller = ActorController(
                actor=self.actor,
                obs_encoder=self.obs_encoder,
                world_model=self.world_model,
            )

        self.control_frequency = control_frequency

    def run(self, env):
        """Main control loop."""
        obs = env.reset()
        self.controller.reset()

        while True:
            start_time = time.perf_counter()

            # Get action
            with torch.no_grad():
                action = self.controller.get_action(
                    torch.tensor(obs).unsqueeze(0).cuda()
                )

            # Execute
            obs, reward, done, info = env.step(action.cpu().numpy())

            if done:
                obs = env.reset()
                self.controller.reset()

            # Maintain control frequency
            elapsed = time.perf_counter() - start_time
            sleep_time = 1.0 / self.control_frequency - elapsed
            if sleep_time > 0:
                time.sleep(sleep_time)


class LatencyOptimizedWorldModel:
    """
    Optimizations for low-latency world model inference.
    """

    @staticmethod
    def optimize(model: nn.Module) -> nn.Module:
        """Apply latency optimizations."""

        # 1. TorchScript compilation
        model = torch.jit.script(model)
        model = torch.jit.freeze(model)

        # 2. Enable CUDA graphs (for fixed input sizes)
        # model = torch.cuda.make_graphed_callables(model, (example_input,))

        # 3. Mixed precision
        model = model.half()

        return model

    @staticmethod
    def benchmark_latency(
        model: nn.Module,
        input_shape: Tuple[int, ...],
        num_iterations: int = 100,
    ) -> Dict[str, float]:
        """Benchmark inference latency."""

        dummy_input = torch.randn(*input_shape).cuda().half()

        # Warmup
        for _ in range(10):
            _ = model(dummy_input)

        # Benchmark
        latencies = []
        for _ in range(num_iterations):
            torch.cuda.synchronize()
            start = time.perf_counter()

            with torch.no_grad():
                _ = model(dummy_input)

            torch.cuda.synchronize()
            latencies.append((time.perf_counter() - start) * 1000)

        return {
            "mean_ms": np.mean(latencies),
            "std_ms": np.std(latencies),
            "p50_ms": np.percentile(latencies, 50),
            "p95_ms": np.percentile(latencies, 95),
            "p99_ms": np.percentile(latencies, 99),
        }
```

---

## Evaluation and Benchmarks

### World Model Evaluation

```python
class WorldModelEvaluator:
    """
    Comprehensive evaluation for world models.
    """

    def evaluate(
        self,
        world_model: RSSM,
        test_dataset: TrajectoryDataset,
    ) -> Dict[str, float]:
        """
        Evaluate world model on multiple metrics.
        """
        metrics = {}

        # Reconstruction quality
        metrics.update(self._evaluate_reconstruction(world_model, test_dataset))

        # Prediction accuracy
        metrics.update(self._evaluate_prediction(world_model, test_dataset))

        # Latent space quality
        metrics.update(self._evaluate_latent_space(world_model, test_dataset))

        return metrics

    def _evaluate_reconstruction(
        self,
        world_model: RSSM,
        dataset: TrajectoryDataset,
    ) -> Dict[str, float]:
        """Evaluate reconstruction quality."""
        total_mse = 0.0
        total_samples = 0

        for batch in dataset:
            obs = batch["observations"]
            actions = batch["actions"]

            # Encode and decode
            latent = world_model.encode_sequence(obs, actions)
            full_state = torch.cat([
                latent["posterior_state"],
                latent["posterior_stochastic"],
            ], dim=-1)
            recon = self.obs_decoder(full_state)

            mse = F.mse_loss(recon, obs, reduction='sum')
            total_mse += mse.item()
            total_samples += obs.numel()

        return {
            "reconstruction/mse": total_mse / total_samples,
            "reconstruction/psnr": 10 * np.log10(1.0 / (total_mse / total_samples)),
        }

    def _evaluate_prediction(
        self,
        world_model: RSSM,
        dataset: TrajectoryDataset,
        prediction_horizons: List[int] = [1, 5, 10, 20],
    ) -> Dict[str, float]:
        """Evaluate multi-step prediction accuracy."""
        metrics = {}

        for horizon in prediction_horizons:
            total_error = 0.0
            total_samples = 0

            for batch in dataset:
                obs = batch["observations"]
                actions = batch["actions"]

                # Split into past and future
                past_obs = obs[:, :-horizon]
                past_actions = actions[:, :-horizon]
                future_obs = obs[:, -horizon:]
                future_actions = actions[:, -horizon:]

                # Encode past
                latent = world_model.encode_sequence(past_obs, past_actions)
                current_state = latent["posterior_state"][:, -1]
                current_stochastic = latent["posterior_stochastic"][:, -1]

                # Predict future
                predicted_states = []
                for t in range(horizon):
                    next_state, next_stochastic = world_model.imagine_step(
                        current_state,
                        current_stochastic,
                        future_actions[:, t],
                    )
                    predicted_states.append(next_state)
                    current_state = next_state
                    current_stochastic = next_stochastic

                predicted_states = torch.stack(predicted_states, dim=1)

                # Encode future for comparison
                future_latent = world_model.encode_sequence(future_obs, future_actions)
                actual_states = future_latent["posterior_state"]

                # Compute error
                error = F.mse_loss(predicted_states, actual_states, reduction='sum')
                total_error += error.item()
                total_samples += actual_states.numel()

            metrics[f"prediction/mse_{horizon}step"] = total_error / total_samples

        return metrics

    def _evaluate_latent_space(
        self,
        world_model: RSSM,
        dataset: TrajectoryDataset,
    ) -> Dict[str, float]:
        """Evaluate latent space properties."""
        all_states = []
        all_stochastics = []

        for batch in dataset:
            obs = batch["observations"]
            actions = batch["actions"]

            latent = world_model.encode_sequence(obs, actions)
            all_states.append(latent["posterior_state"].reshape(-1, latent["posterior_state"].shape[-1]))
            all_stochastics.append(latent["posterior_stochastic"].reshape(-1, latent["posterior_stochastic"].shape[-1]))

        states = torch.cat(all_states, dim=0)
        stochastics = torch.cat(all_stochastics, dim=0)

        # Compute statistics
        state_mean = states.mean(dim=0)
        state_std = states.std(dim=0)

        return {
            "latent/state_mean_norm": state_mean.norm().item(),
            "latent/state_std_mean": state_std.mean().item(),
            "latent/stochastic_entropy": -torch.distributions.Normal(
                stochastics.mean(dim=0),
                stochastics.std(dim=0) + 1e-6,
            ).entropy().mean().item(),
        }
```

### Benchmark Results

```
+====================================================================================+
|                       WORLD MODEL BENCHMARK RESULTS                                 |
+====================================================================================+
|                                                                                     |
| Dataset: D4RL (MuJoCo)                                                             |
|                                                                                     |
| Model              | 1-Step MSE | 10-Step MSE | 50-Step MSE | Policy Return        |
| ------------------|------------|-------------|-------------|----------------------|
| RSSM-Small         | 0.012      | 0.089       | 0.342       | 72.3  4.2          |
| RSSM-Medium        | 0.008      | 0.056       | 0.234       | 81.5  3.8          |
| RSSM-Large         | 0.005      | 0.038       | 0.167       | 87.2  2.9          |
| Ensemble (5x)      | 0.004      | 0.031       | 0.145       | 89.1  2.5          |
| DreamerV3          | 0.003      | 0.025       | 0.112       | 92.4  2.1          |
|                                                                                     |
| Imagination Efficiency:                                                             |
| - Training speedup with imagination: 5-10x                                          |
| - Sample efficiency improvement: 10-20x                                             |
| - Planning latency: 5-15ms (MPPI), 10-30ms (CEM)                                   |
|                                                                                     |
+====================================================================================+
```

---

## Summary

This guide covered the complete training pipeline for temporal and world model VLA:

1. **Stage 1**: Temporal encoder training (history encoding, contrastive learning)
2. **Stage 2**: World model training (RSSM, latent dynamics)
3. **Stage 3**: Latent dynamics learning (deterministic, stochastic, ensemble)
4. **Stage 4**: Imagination-based planning (CEM, MPPI)
5. **Stage 5**: Model predictive control (MPC, receding horizon)
6. **Stage 6**: Policy learning with world models (Dreamer-style actor-critic)

**Key recommendations:**
- Use transformer-based temporal encoders for complex sequences
- Train world models with KL regularization and free bits
- Use ensemble for uncertainty quantification
- Apply CEM or MPPI for planning in latent space
- Fine-tune actor-critic on imagined trajectories for sample efficiency
- Optimize for latency in real-time deployments

---

## Datasets Used for Each Training Step

| Training Stage | Dataset | Public Source | Description |
|----------------|---------|---------------|-------------|
| **Stage 1: Temporal Encoder Training** | Robot trajectory datasets | [jxu124/OpenX-Embodiment](https://huggingface.co/datasets/jxu124/OpenX-Embodiment) | Sequential observation data for history encoding |
| **Stage 2: World Model Training** | D4RL MuJoCo | [imone/D4RL](https://huggingface.co/datasets/imone/D4RL) | DMControl benchmark tasks for dynamics learning |
| **Stage 2: World Model Training** | Visual D4RL (VD4RL) | [conglu/vd4rl](https://huggingface.co/datasets/conglu/vd4rl) | Pixel-based offline RL benchmarks |
| **Stage 3: Latent Dynamics Learning** | Robot manipulation data | [lerobot on HuggingFace](https://huggingface.co/lerobot) | Action-conditioned state transitions |
| **Stage 4a: Online RL (Planning)** | MuJoCo/Isaac Gym | [mujoco.org](https://mujoco.org/) / [isaac-gym](https://developer.nvidia.com/isaac-gym) | Real-time simulation for imagination-based PPO/SAC training |
| **Stage 4a: Online RL (MPC)** | Online rollouts | Varies | MPC trajectory optimization with real-time world model |
| **Stage 4b: Offline RL (Planning)** | D4RL | [imone/D4RL](https://huggingface.co/datasets/imone/D4RL) | Offline data for CQL/IQL imagination-based training |
| **Stage 4b: Offline RL (Planning)** | Visual D4RL (VD4RL) | [conglu/vd4rl](https://huggingface.co/datasets/conglu/vd4rl) | Pixel-based offline trajectories for Decision Transformer |
| **Stage 5: Policy Learning** | Imagined trajectories | Varies | Dyna-style training with world model |
| **Evaluation** | D4RL MuJoCo | [imone/D4RL](https://huggingface.co/datasets/imone/D4RL) | Walker, Cheetah, Humanoid benchmarks |
| **Evaluation** | MetaWorld | [lerobot/metaworld_mt50](https://huggingface.co/datasets/lerobot/metaworld_mt50) | Multi-task manipulation |
| **Video Prediction** | RoboNet | [jxu124/OpenX-Embodiment](https://huggingface.co/datasets/jxu124/OpenX-Embodiment) (subset) | Robot video prediction |
| **Video Prediction** | BAIR robot pushing | [tensorflow.org/datasets](https://www.tensorflow.org/datasets/catalog/bair_robot_pushing_small) | Robot pushing video prediction |

---

## Related Documents

- [Training Pipeline Overview](training_pipeline_overview.md)
- [Autonomous Vehicle Training](training_autonomous_vehicle.md)
- [Humanoid Robot Training](training_humanoid.md)
- [Architecture Guide](architecture.md)
- [Training Recipes](training_recipes.md)
